<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="generator" content="VuePress 2.0.0-beta.49">
    <style>
      :root {
        --c-bg: #fff;
      }
      html.dark {
        --c-bg: #22272e;
      }
      html, body {
        background-color: var(--c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme');
			const systemDarkMode = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
			if (userMode === 'dark' || (userMode !== 'light' && systemDarkMode)) {
				document.documentElement.classList.toggle('dark', true);
			}
    </script>
    <link rel="icon" type="image/png" sizes="16x16" href="/img/logo/favicon-16x16.png"><link rel="icon" type="image/png" sizes="32x32" href="/img/logo/favicon-32x32.png"><meta name="application-name" content="Evarle"><meta name="apple-mobile-web-app-title" content="Evarle"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="apple-touch-icon" href="/img/logo/apple-touch-icon.png"><meta name="theme-color" content="#377bb5"><meta name="msapplication-TileColor" content="#377bb5"><title>使用so-vits-svc进行语音模型训练 | Evarle zhuo</title><meta name="description" content="A student who likes coding. ">
    <link rel="modulepreload" href="/assets/app.1ce497e0.js"><link rel="modulepreload" href="/assets/index.html.f54beede.js"><link rel="modulepreload" href="/assets/index.html.3de6c6dc.js"><link rel="prefetch" href="/assets/index.html.aa899b3a.js"><link rel="prefetch" href="/assets/index.html.ea93dd39.js"><link rel="prefetch" href="/assets/index.html.ea63ce6d.js"><link rel="prefetch" href="/assets/index.html.c6a26f93.js"><link rel="prefetch" href="/assets/index.html.3d723756.js"><link rel="prefetch" href="/assets/index.html.047a422d.js"><link rel="prefetch" href="/assets/index.html.93dec09b.js"><link rel="prefetch" href="/assets/404.html.7c518b82.js"><link rel="prefetch" href="/assets/index.html.28fa388e.js"><link rel="prefetch" href="/assets/index.html.271c9982.js"><link rel="prefetch" href="/assets/index.html.51da7a59.js"><link rel="prefetch" href="/assets/index.html.40df44ff.js"><link rel="prefetch" href="/assets/index.html.e40641a1.js"><link rel="prefetch" href="/assets/index.html.b251f7d6.js"><link rel="prefetch" href="/assets/index.html.459b8aa1.js"><link rel="prefetch" href="/assets/index.html.e07d945d.js"><link rel="prefetch" href="/assets/index.html.c588c220.js"><link rel="prefetch" href="/assets/index.html.a906e6ca.js"><link rel="prefetch" href="/assets/index.html.3f1a431e.js"><link rel="prefetch" href="/assets/index.html.7677a516.js"><link rel="prefetch" href="/assets/index.html.ceb08931.js"><link rel="prefetch" href="/assets/index.html.003630b5.js"><link rel="prefetch" href="/assets/index.html.7df08b6d.js"><link rel="prefetch" href="/assets/404.html.e58de7d8.js"><link rel="prefetch" href="/assets/index.html.f248c9e7.js"><link rel="prefetch" href="/assets/index.html.e5b50828.js"><link rel="prefetch" href="/assets/index.html.397cac90.js"><link rel="prefetch" href="/assets/index.html.a3c64bd6.js"><link rel="prefetch" href="/assets/index.html.5f7826aa.js"><link rel="prefetch" href="/assets/index.html.0b550215.js"><link rel="prefetch" href="/assets/index.html.9c1a7e2c.js"><link rel="prefetch" href="/assets/index.html.07869bcb.js"><link rel="prefetch" href="/assets/404.00393b26.js"><link rel="prefetch" href="/assets/HomePage.aeeb4a89.js"><link rel="prefetch" href="/assets/Layout.e118b282.js"><link rel="prefetch" href="/assets/Links.e63ecc88.js"><link rel="prefetch" href="/assets/Post.502dc6aa.js"><link rel="prefetch" href="/assets/Tags.113ff90a.js">
    <link rel="stylesheet" href="/assets/style.5ec59d18.css">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme-container no-sidebar"><!--[--><header class="navbar invert"><span><a href="/" class=""><span class="site-name">Evarle</span></a></span><div class="navbar-items-wrapper" style=""><!--[--><!--]--><nav class="navbar-items can-hide"><!--[--><div class="navbar-item"><a href="/" class="" aria-label="Home"><!--[--><!--]--><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="0 0 512 512" fill="currentColor"><path d="M261.56 101.28a8 8 0 00-11.06 0L66.4 277.15a8 8 0 00-2.47 5.79L63.9 448a32 32 0 0032 32H192a16 16 0 0016-16V328a8 8 0 018-8h80a8 8 0 018 8v136a16 16 0 0016 16h96.06a32 32 0 0032-32V282.94a8 8 0 00-2.47-5.79z"/><path d="M490.91 244.15l-74.8-71.56V64a16 16 0 00-16-16h-48a16 16 0 00-16 16v32l-57.92-55.38C272.77 35.14 264.71 32 256 32c-8.68 0-16.72 3.14-22.14 8.63l-212.7 203.5c-6.22 6-7 15.87-1.34 22.37A16 16 0 0043 267.56L250.5 69.28a8 8 0 0111.06 0l207.52 198.28a16 16 0 0022.59-.44c6.14-6.36 5.63-16.86-.76-22.97z"/></svg></span><span>Home</span><!--[--><!--]--></a></div><div class="navbar-item"><a href="/tags/" class="" aria-label="Tags"><!--[--><!--]--><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 010 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg></span><span>Tags</span><!--[--><!--]--></a></div><div class="navbar-item"><a href="/links/" class="" aria-label="Links"><!--[--><!--]--><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></span><span>Links</span><!--[--><!--]--></a></div><div class="navbar-item"><a class="external-link" href="https://evarle.top/about/" rel="noopener noreferrer" target="_blank" aria-label="About"><!--[--><!--]--><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M256 288c79.5 0 144-64.5 144-144S335.5 0 256 0 112 64.5 112 144s64.5 144 144 144zm128 32h-55.1c-22.2 10.2-46.9 16-72.9 16s-50.6-5.8-72.9-16H128C57.3 320 0 377.3 0 448v16c0 26.5 21.5 48 48 48h416c26.5 0 48-21.5 48-48v-16c0-70.7-57.3-128-128-128z"/></svg></span><span>About</span><!--[--><!--]--></a></div><!--]--><div class="navbar-item"><a style="cursor:pointer;"><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="0 0 24 24" fill="currentColor"><path fill="none" d="M0 0h24v24H0z"/><path d="M11 2c4.968 0 9 4.032 9 9s-4.032 9-9 9-9-4.032-9-9 4.032-9 9-9zm0 16c3.867 0 7-3.133 7-7 0-3.868-3.133-7-7-7-3.868 0-7 3.132-7 7 0 3.867 3.132 7 7 7zm8.485.071l2.829 2.828-1.415 1.415-2.828-2.829 1.414-1.414z"/></svg></span><span>Search</span></a></div></nav><!--[--><!--]--><!----></div></header><!--]--><div class="sidebar-mask"></div><!--[--><aside class="sidebar"><nav class="navbar-items"><!--[--><div class="navbar-item"><a href="/" class="" aria-label="Home"><!--[--><!--]--><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="0 0 512 512" fill="currentColor"><path d="M261.56 101.28a8 8 0 00-11.06 0L66.4 277.15a8 8 0 00-2.47 5.79L63.9 448a32 32 0 0032 32H192a16 16 0 0016-16V328a8 8 0 018-8h80a8 8 0 018 8v136a16 16 0 0016 16h96.06a32 32 0 0032-32V282.94a8 8 0 00-2.47-5.79z"/><path d="M490.91 244.15l-74.8-71.56V64a16 16 0 00-16-16h-48a16 16 0 00-16 16v32l-57.92-55.38C272.77 35.14 264.71 32 256 32c-8.68 0-16.72 3.14-22.14 8.63l-212.7 203.5c-6.22 6-7 15.87-1.34 22.37A16 16 0 0043 267.56L250.5 69.28a8 8 0 0111.06 0l207.52 198.28a16 16 0 0022.59-.44c6.14-6.36 5.63-16.86-.76-22.97z"/></svg></span><span>Home</span><!--[--><!--]--></a></div><div class="navbar-item"><a href="/tags/" class="" aria-label="Tags"><!--[--><!--]--><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 010 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg></span><span>Tags</span><!--[--><!--]--></a></div><div class="navbar-item"><a href="/links/" class="" aria-label="Links"><!--[--><!--]--><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></span><span>Links</span><!--[--><!--]--></a></div><div class="navbar-item"><a class="external-link" href="https://evarle.top/about/" rel="noopener noreferrer" target="_blank" aria-label="About"><!--[--><!--]--><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M256 288c79.5 0 144-64.5 144-144S335.5 0 256 0 112 64.5 112 144s64.5 144 144 144zm128 32h-55.1c-22.2 10.2-46.9 16-72.9 16s-50.6-5.8-72.9-16H128C57.3 320 0 377.3 0 448v16c0 26.5 21.5 48 48 48h416c26.5 0 48-21.5 48-48v-16c0-70.7-57.3-128-128-128z"/></svg></span><span>About</span><!--[--><!--]--></a></div><!--]--><div class="navbar-item"><a style="cursor:pointer;"><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="0 0 24 24" fill="currentColor"><path fill="none" d="M0 0h24v24H0z"/><path d="M11 2c4.968 0 9 4.032 9 9s-4.032 9-9 9-9-4.032-9-9 4.032-9 9-9zm0 16c3.867 0 7-3.133 7-7 0-3.868-3.133-7-7-7-3.868 0-7 3.132-7 7 0 3.867 3.132 7 7 7zm8.485.071l2.829 2.828-1.415 1.415-2.828-2.829 1.414-1.414z"/></svg></span><span>Search</span></a></div></nav><!--[--><!--]--><!----><!--[--><!--]--></aside><!--]--><div class="page-content"><!--[--><div class="show-catalog post-wrapper"><div class="article-header use-image post-header" style="background-image:url(/img/in-post/2023-11-30/header.jpg);"><div class="article-header-mask" style="background:rgb(67, 65, 47, .2);"></div><div class="article-header-content"><div class="article-tags"><!--[--><a href="/tags/%E7%AC%94%E8%AE%B0/" class="article-tag">笔记</a><a href="/tags/ai/" class="article-tag">AI</a><!--]--></div><h1 class="article-title">使用so-vits-svc进行语音模型训练</h1><p class="article-subtitle">Linux AI</p><div class="article-icons"><div class="article-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="-75.52 -43.52 599.04 599.04" fill="currentColor"><path d="M313.6 304c-28.7 0-42.5 16-89.6 16-47.1 0-60.8-16-89.6-16C60.2 304 0 364.2 0 438.4V464c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48v-25.6c0-74.2-60.2-134.4-134.4-134.4zM400 464H48v-25.6c0-47.6 38.8-86.4 86.4-86.4 14.6 0 38.3 16 89.6 16 51.7 0 74.9-16 89.6-16 47.6 0 86.4 38.8 86.4 86.4V464zM224 288c79.5 0 144-64.5 144-144S303.5 0 224 0 80 64.5 80 144s64.5 144 144 144zm0-240c52.9 0 96 43.1 96 96s-43.1 96-96 96-96-43.1-96-96 43.1-96 96-96z"/></svg><span>Evarle</span></div><div class="article-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="-75.52 -43.52 599.04 599.04" fill="currentColor"><path d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"/></svg><span>2023-11-30</span></div><div class="article-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="0 0 24 24" fill="currentColor"><path fill="none" d="M0 0h24v24H0z"/><path d="M17.618 5.968l1.453-1.453 1.414 1.414-1.453 1.453a9 9 0 11-1.414-1.414zM12 20a7 7 0 100-14 7 7 0 000 14zM11 8h2v6h-2V8zM8 1h8v2H8V1z"/></svg><span>19 min</span></div></div></div><!----></div><main class="page post-content"><!--[--><!--]--><div class="theme-gungnir-content"><!--[--><!--]--><div><p>训练属于自己的AI模型，使用so-vits-svc进行语音模型训练。用AI声音模型进行推理，让你想要的声音唱你喜欢的歌。</p><hr><p>所用项目是so-vits-svc：https://github.com/svc-develop-team/so-vits-svc</p><p>训练模型需要遵循官方的使用规范。</p><h2 id="模型简介" tabindex="-1"><a class="header-anchor" href="#模型简介" aria-hidden="true">#</a> 模型简介</h2><p>歌声音色转换模型，通过 SoftVC 内容编码器提取源音频语音特征，与 F0 同时输入 VITS 替换原本的文本输入达到歌声转换的效果。同时，更换声码器为 <a href="https://github.com/openvpi/DiffSinger/tree/refactor/modules/nsf_hifigan" target="_blank" rel="noopener noreferrer">NSF HiFiGAN</a> 解决断音问题。</p><h2 id="使用规约" tabindex="-1"><a class="header-anchor" href="#使用规约" aria-hidden="true">#</a> 使用规约</h2><p><strong>Warning：请自行解决数据集授权问题，禁止使用非授权数据集进行训练！任何由于使用非授权数据集进行训练造成的问题，需自行承担全部责任和后果！与仓库、仓库维护者、svc develop team 无关！</strong></p><ol><li>本项目是基于学术交流目的建立，仅供交流与学习使用，并非为生产环境准备。</li><li>任何发布到视频平台的基于 sovits 制作的视频，都必须要在简介明确指明用于变声器转换的输入源歌声、音频，例如：使用他人发布的视频 / 音频，通过分离的人声作为输入源进行转换的，必须要给出明确的原视频、音乐链接；若使用是自己的人声，或是使用其他歌声合成引擎合成的声音作为输入源进行转换的，也必须在简介加以说明。</li><li>由输入源造成的侵权问题需自行承担全部责任和一切后果。使用其他商用歌声合成软件作为输入源时，请确保遵守该软件的使用条例，注意，许多歌声合成引擎使用条例中明确指明不可用于输入源进行转换！</li><li>禁止使用该项目从事违法行为与宗教、政治等活动，该项目维护者坚决抵制上述行为，不同意此条则禁止使用该项目。</li><li>继续使用视为已同意本仓库 README 所述相关条例，本仓库 README 已进行劝导义务，不对后续可能存在问题负责。</li><li>如果将此项目用于任何其他企划，请提前联系并告知本仓库作者，十分感谢。</li></ol><h2 id="安装wsl2和conda" tabindex="-1"><a class="header-anchor" href="#安装wsl2和conda" aria-hidden="true">#</a> 安装wsl2和conda</h2><p>详细安装可参考之前的文章：<a href="https://evarle.one/post/2023/10/14/Linux-and-wsl/" target="_blank" rel="noopener noreferrer">Linux基础和wsl安装</a></p><h3 id="为什么用linux系统来训练ai模型" tabindex="-1"><a class="header-anchor" href="#为什么用linux系统来训练ai模型" aria-hidden="true">#</a> 为什么用Linux系统来训练AI模型</h3><p>虽然Windows系统也可以用来训练AI模型，但是Linux系统相对于Windows系统更具优势。Linux的开源环境提供了更好的深度学习框架和工具支持，系统架构在高负载和多任务处理方面更为稳定，灵活的命令行工具和远程访问使用户能够更便捷地管理和监控任务，优秀的资源管理和GPU支持提高了训练效率，可以持续稳定长时间地训练AI模型，达到更好的模型效果。而且Linux环境对于小白更加友好，不容易出现依赖安装失败或者报错等问题。</p><h3 id="为什么使用conda" tabindex="-1"><a class="header-anchor" href="#为什么使用conda" aria-hidden="true">#</a> 为什么使用conda</h3><p>Conda（Anaconda）是一个用于管理和安装python软件包的工具。它让你可以轻松创建、分享、导出和安装项目的环境，以及安装科学计算和机器学习库，简化了依赖管理的过程。 由于不同项目所需的python版本还有pip依赖环境的不同，conda可以很好的管理多版本不同环境依赖的python环境，用于不同AI项目的训练。</p><blockquote><p>AI模型训练对GPU性能要求高，推荐使用6G以上显存的英伟达显卡（如RTX3060）。</p><p>在Windows中开启虚拟内存，推荐选择固态硬盘开启16GB以上虚拟内存。具体位置在 此电脑-属性-高级系统设置-高级-性能-高级中的虚拟内存。</p></blockquote><h2 id="环境依赖准备" tabindex="-1"><a class="header-anchor" href="#环境依赖准备" aria-hidden="true">#</a> 环境依赖准备</h2><h3 id="python" tabindex="-1"><a class="header-anchor" href="#python" aria-hidden="true">#</a> Python</h3><blockquote><p>在进行测试后，我们认为<code>Python 3.8.9</code>能够稳定地运行该项目.</p></blockquote><div class="language-bash ext-sh"><pre class="language-bash"><code>conda create --name python3.8 <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.8</span> <span class="token comment">#创建python3.8的conda环境</span>
conda activate python3.8 <span class="token comment">#激活conda环境</span>
</code></pre></div><h3 id="cuda" tabindex="-1"><a class="header-anchor" href="#cuda" aria-hidden="true">#</a> CUDA</h3><p>在命令行中输入<code>nvidia-smi</code>显示显卡信息。</p><p>在官网安装<a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener noreferrer">cuda-toolkit</a>，选择Linux对应Linux系统版本，选择network版本，然后按照命令一句句执行安装。若显卡cuda版本过低，需要安装其他版本的https://developer.nvidia.com/cuda-toolkit-archive在这里选择安装对应版本。</p><div class="language-bash ext-sh"><pre class="language-bash"><code><span class="token function">wget</span> https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb
<span class="token function">sudo</span> dpkg -i cuda-keyring_1.1-1_all.deb
<span class="token function">sudo</span> <span class="token function">apt-get</span> update
<span class="token function">sudo</span> <span class="token function">apt-get</span> -y <span class="token function">install</span> cuda-toolkit-12-3
</code></pre></div><h3 id="pytorch" tabindex="-1"><a class="header-anchor" href="#pytorch" aria-hidden="true">#</a> Pytorch</h3><p>根据对应的cuda版本选择安装Pytorch版本，https://pytorch.org/get-started/locally/，选择对应版本安装。注意，高版本的cuda能兼容低版本的pytorch。</p><div class="language-bash ext-sh"><pre class="language-bash"><code>conda <span class="token function">install</span> pytorch torchvision torchaudio pytorch-cuda<span class="token operator">=</span><span class="token number">12.1</span> -c pytorch -c nvidia
</code></pre></div><h3 id="测试环境" tabindex="-1"><a class="header-anchor" href="#测试环境" aria-hidden="true">#</a> 测试环境</h3><div class="language-python ext-py"><pre class="language-python"><code>python
<span class="token comment"># 回车运行</span>
<span class="token keyword">import</span> torch
<span class="token comment"># 回车运行</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span>
<span class="token comment"># 回车运行</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 回车运行，显示true代表版本匹配</span>
</code></pre></div><h2 id="配置项目" tabindex="-1"><a class="header-anchor" href="#配置项目" aria-hidden="true">#</a> 配置项目</h2><ul><li><p>使用 <code>git clone https://github.com/svc-develop-team/so-vits-svc.git</code>克隆项目到本地。或者直接在项目页面download zip，下载源码再解压放置到特定文件夹。</p></li><li><p>进入项目文件夹 <code>cd so-vits-svc</code>。</p></li><li><p>下载项目所需的pip依赖 <code>pip install -r requirements.txt</code>。</p></li><li><p>根据<a href="https://github.com/svc-develop-team/so-vits-svc/blob/4.1-Stable/README_zh_CN.md" target="_blank" rel="noopener noreferrer">官方文档</a>，进行模型文件预下载、数据集准备、数据预处理、训练等。</p></li></ul><h2 id="预下载模型文件" tabindex="-1"><a class="header-anchor" href="#预下载模型文件" aria-hidden="true">#</a> 预下载模型文件</h2><h4 id="必须项" tabindex="-1"><a class="header-anchor" href="#必须项" aria-hidden="true">#</a> <strong>必须项</strong></h4><p><strong>以下编码器需要选择一个使用</strong></p><h5 id="_1-若使用-contentvec-作为声音编码器-推荐" tabindex="-1"><a class="header-anchor" href="#_1-若使用-contentvec-作为声音编码器-推荐" aria-hidden="true">#</a> <strong>1. 若使用 contentvec 作为声音编码器（推荐）</strong></h5><p><code>vec768l12</code>与<code>vec256l9</code> 需要该编码器</p><ul><li>contentvec ：<a href="https://ibm.box.com/s/z1wgl1stco8ffooyatzdwsqn2psd9lrr" target="_blank" rel="noopener noreferrer">checkpoint_best_legacy_500.pt</a><ul><li>放在<code>pretrain</code>目录下</li></ul></li></ul><p>或者下载下面的 ContentVec，大小只有 199MB，但效果相同:</p><ul><li>contentvec ：<a href="https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt" target="_blank" rel="noopener noreferrer">hubert_base.pt</a><ul><li>将文件名改为<code>checkpoint_best_legacy_500.pt</code>后，放在<code>pretrain</code>目录下</li></ul></li></ul><div class="language-bash ext-sh"><pre class="language-bash"><code><span class="token comment"># contentvec</span>
<span class="token function">wget</span> -P pretrain/ http://obs.cstcloud.cn/share/obs/sankagenkeshi/checkpoint_best_legacy_500.pt
<span class="token comment"># 也可手动下载放在pretrain目录</span>
</code></pre></div><h5 id="_2-若使用-hubertsoft-作为声音编码器" tabindex="-1"><a class="header-anchor" href="#_2-若使用-hubertsoft-作为声音编码器" aria-hidden="true">#</a> <strong>2. 若使用 hubertsoft 作为声音编码器</strong></h5><ul><li>soft vc hubert：<a href="https://github.com/bshall/hubert/releases/download/v0.1/hubert-soft-0d54a1f4.pt" target="_blank" rel="noopener noreferrer">hubert-soft-0d54a1f4.pt</a><ul><li>放在<code>pretrain</code>目录下</li></ul></li></ul><h5 id="_3-若使用-whisper-ppg-作为声音编码器" tabindex="-1"><a class="header-anchor" href="#_3-若使用-whisper-ppg-作为声音编码器" aria-hidden="true">#</a> <strong>3. 若使用 Whisper-ppg 作为声音编码器</strong></h5><ul><li>下载模型 <a href="https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt" target="_blank" rel="noopener noreferrer">medium.pt</a>, 该模型适配<code>whisper-ppg</code></li><li>下载模型 <a href="https://openaipublic.azureedge.net/main/whisper/models/81f7c96c852ee8fc832187b0132e569d6c3065a3252ed18e56effd0b6a73e524/large-v2.pt" target="_blank" rel="noopener noreferrer">large-v2.pt</a>, 该模型适配<code>whisper-ppg-large</code><ul><li>放在<code>pretrain</code>目录下</li></ul></li></ul><h5 id="_4-若使用-cnhubertlarge-作为声音编码器" tabindex="-1"><a class="header-anchor" href="#_4-若使用-cnhubertlarge-作为声音编码器" aria-hidden="true">#</a> <strong>4. 若使用 cnhubertlarge 作为声音编码器</strong></h5><ul><li>下载模型 <a href="https://huggingface.co/TencentGameMate/chinese-hubert-large/resolve/main/chinese-hubert-large-fairseq-ckpt.pt" target="_blank" rel="noopener noreferrer">chinese-hubert-large-fairseq-ckpt.pt</a><ul><li>放在<code>pretrain</code>目录下</li></ul></li></ul><h5 id="_5-若使用-dphubert-作为声音编码器" tabindex="-1"><a class="header-anchor" href="#_5-若使用-dphubert-作为声音编码器" aria-hidden="true">#</a> <strong>5. 若使用 dphubert 作为声音编码器</strong></h5><ul><li>下载模型 <a href="https://huggingface.co/pyf98/DPHuBERT/resolve/main/DPHuBERT-sp0.75.pth" target="_blank" rel="noopener noreferrer">DPHuBERT-sp0.75.pth</a><ul><li>放在<code>pretrain</code>目录下</li></ul></li></ul><h5 id="_6-若使用-onnxhubert-contentvec-作为声音编码器" tabindex="-1"><a class="header-anchor" href="#_6-若使用-onnxhubert-contentvec-作为声音编码器" aria-hidden="true">#</a> <strong>6. 若使用 OnnxHubert/ContentVec 作为声音编码器</strong></h5><ul><li>下载模型 <a href="https://huggingface.co/NaruseMioShirakana/MoeSS-SUBModel/tree/main" target="_blank" rel="noopener noreferrer">MoeSS-SUBModel</a><ul><li>放在<code>pretrain</code>目录下</li></ul></li></ul><h4 id="编码器列表" tabindex="-1"><a class="header-anchor" href="#编码器列表" aria-hidden="true">#</a> <strong>编码器列表</strong></h4><ul><li>&quot;vec768l12&quot;</li></ul><ul><li>&quot;vec256l9&quot;</li><li>&quot;vec256l9-onnx&quot;</li><li>&quot;vec256l12-onnx&quot;</li><li>&quot;vec768l9-onnx&quot;</li><li>&quot;vec768l12-onnx&quot;</li><li>&quot;hubertsoft-onnx&quot;</li><li>&quot;hubertsoft&quot;</li><li>&quot;whisper-ppg&quot;</li><li>&quot;cnhubertlarge&quot;</li><li>&quot;dphubert&quot;</li><li>&quot;whisper-ppg-large&quot;</li></ul><h4 id="可选项-强烈建议使用" tabindex="-1"><a class="header-anchor" href="#可选项-强烈建议使用" aria-hidden="true">#</a> <strong>可选项(强烈建议使用)</strong></h4><ul><li><p>预训练底模文件： <code>G_0.pth</code> <code>D_0.pth</code></p><ul><li>放在<code>logs/44k</code>目录下</li></ul></li><li><p>扩散模型预训练底模文件： <code>model_0.pt</code></p><ul><li>放在<code>logs/44k/diffusion</code>目录下</li></ul></li></ul><p>扩散模型引用了<a href="https://github.com/yxlllc/DDSP-SVC" target="_blank" rel="noopener noreferrer">DDSP-SVC</a>的 Diffusion Model，底模与<a href="https://github.com/yxlllc/DDSP-SVC" target="_blank" rel="noopener noreferrer">DDSP-SVC</a>的扩散模型底模通用，可以去<a href="https://github.com/yxlllc/DDSP-SVC" target="_blank" rel="noopener noreferrer">DDSP-SVC</a>获取扩散模型的底模</p><p>虽然底模一般不会引起什么版权问题，但还是请注意一下，比如事先询问作者，又或者作者在模型描述中明确写明了可行的用途。</p><h2 id="数据集准备" tabindex="-1"><a class="header-anchor" href="#数据集准备" aria-hidden="true">#</a> 数据集准备</h2><p>仅需要以以下文件结构将数据集放入 dataset_raw 目录即可。</p><div class="language-text ext-text"><pre class="language-text"><code>dataset_raw
├───speaker0
│   ├───xxx1-xxx1.wav
│   ├───...
│   └───Lxx-0xx8.wav
└───speaker1
    ├───xx2-0xxx2.wav
    ├───...
    └───xxx7-xxx007.wav
</code></pre></div><p>对于每一个音频文件的名称并没有格式的限制(<code>000001.wav</code>~<code>999999.wav</code>之类的命名方式也是合法的)，不过文件类型必须是<code>wav</code>。</p><p>可以自定义说话人名称</p><div class="language-text ext-text"><pre class="language-text"><code>dataset_raw
└───suijiSUI
    ├───1.wav
    ├───...
    └───25788785-20221210-200143-856_01_(Vocals)_0_0.wav
</code></pre></div><h2 id="🛠️-数据预处理" tabindex="-1"><a class="header-anchor" href="#🛠️-数据预处理" aria-hidden="true">#</a> 🛠️ 数据预处理</h2><h3 id="_0-音频切片" tabindex="-1"><a class="header-anchor" href="#_0-音频切片" aria-hidden="true">#</a> 0. 音频切片</h3><p>将音频切片至<code>5s - 15s</code>, 稍微长点也无伤大雅，实在太长可能会导致训练中途甚至预处理就爆显存</p><p>可以使用 <a href="https://github.com/flutydeer/audio-slicer" target="_blank" rel="noopener noreferrer">audio-slicer-GUI</a>、<a href="https://github.com/openvpi/audio-slicer" target="_blank" rel="noopener noreferrer">audio-slicer-CLI</a></p><p>一般情况下只需调整其中的<code>Minimum Interval</code>，普通陈述素材通常保持默认即可，歌唱素材可以调整至<code>100</code>甚至<code>50</code></p><p>切完之后手动删除过长过短的音频</p><p><strong>如果你使用 Whisper-ppg 声音编码器进行训练，所有的切片长度必须小于 30s</strong></p><h3 id="_1-重采样至-44100hz-单声道" tabindex="-1"><a class="header-anchor" href="#_1-重采样至-44100hz-单声道" aria-hidden="true">#</a> 1. 重采样至 44100Hz 单声道</h3><div class="language-text ext-text"><pre class="language-text"><code>python resample.py
</code></pre></div><h4 id="注意" tabindex="-1"><a class="header-anchor" href="#注意" aria-hidden="true">#</a> 注意</h4><p>虽然本项目拥有重采样、转换单声道与响度匹配的脚本 resample.py，但是默认的响度匹配是匹配到 0db。这可能会造成音质的受损。而 python 的响度匹配包 pyloudnorm 无法对电平进行压限，这会导致爆音。所以建议可以考虑使用专业声音处理软件如<code>adobe audition</code>等软件做响度匹配处理。若已经使用其他软件做响度匹配，可以在运行上述命令时添加<code>--skip_loudnorm</code>跳过响度匹配步骤。如：</p><div class="language-text ext-text"><pre class="language-text"><code>python resample.py --skip_loudnorm
</code></pre></div><h3 id="_2-自动划分训练集、验证集-以及自动生成配置文件" tabindex="-1"><a class="header-anchor" href="#_2-自动划分训练集、验证集-以及自动生成配置文件" aria-hidden="true">#</a> 2. 自动划分训练集、验证集，以及自动生成配置文件</h3><div class="language-text ext-text"><pre class="language-text"><code>python preprocess_flist_config.py --speech_encoder vec768l12
</code></pre></div><p>speech_encoder 拥有以下选择</p><div class="language-text ext-text"><pre class="language-text"><code>vec768l12
vec256l9
hubertsoft
whisper-ppg
whisper-ppg-large
cnhubertlarge
dphubert
wavlmbase+
</code></pre></div><p>如果省略 speech_encoder 参数，默认值为 vec768l12</p><p><strong>使用响度嵌入</strong></p><p>若使用响度嵌入，需要增加<code>--vol_aug</code>参数，比如：</p><div class="language-text ext-text"><pre class="language-text"><code>python preprocess_flist_config.py --speech_encoder vec768l12 --vol_aug
</code></pre></div><p>使用后训练出的模型将匹配到输入源响度，否则为训练集响度。</p><h4 id="此时可以在生成的-config-json-与-diffusion-yaml-修改部分参数" tabindex="-1"><a class="header-anchor" href="#此时可以在生成的-config-json-与-diffusion-yaml-修改部分参数" aria-hidden="true">#</a> 此时可以在生成的 config.json 与 diffusion.yaml 修改部分参数</h4><h5 id="config-json" tabindex="-1"><a class="header-anchor" href="#config-json" aria-hidden="true">#</a> config.json</h5><ul><li><code>keep_ckpts</code>：训练时保留最后几个模型，<code>0</code>为保留所有，默认只保留最后<code>3</code>个</li><li><code>all_in_mem</code>：加载所有数据集到内存中，某些平台的硬盘 IO 过于低下、同时内存容量 <strong>远大于</strong> 数据集体积时可以启用</li><li><code>batch_size</code>：单次训练加载到 GPU 的数据量，调整到低于显存容量的大小即可</li><li><code>vocoder_name</code> : 选择一种声码器，默认为<code>nsf-hifigan</code>.</li></ul><h5 id="diffusion-yaml" tabindex="-1"><a class="header-anchor" href="#diffusion-yaml" aria-hidden="true">#</a> diffusion.yaml</h5><ul><li><code>cache_all_data</code>：加载所有数据集到内存中，某些平台的硬盘 IO 过于低下、同时内存容量 <strong>远大于</strong> 数据集体积时可以启用</li><li><code>duration</code>：训练时音频切片时长，可根据显存大小调整，<strong>注意，该值必须小于训练集内音频的最短时间！</strong></li><li><code>batch_size</code>：单次训练加载到 GPU 的数据量，调整到低于显存容量的大小即可</li><li><code>timesteps</code> : 扩散模型总步数，默认为 1000.</li><li><code>k_step_max</code> : 训练时可仅训练<code>k_step_max</code>步扩散以节约训练时间，注意，该值必须小于<code>timesteps</code>，0 为训练整个扩散模型，<strong>注意，如果不训练整个扩散模型将无法使用仅扩散模型推理！</strong></li></ul><h5 id="声码器列表" tabindex="-1"><a class="header-anchor" href="#声码器列表" aria-hidden="true">#</a> <strong>声码器列表</strong></h5><div class="language-text ext-text"><pre class="language-text"><code>nsf-hifigan
nsf-snake-hifigan
</code></pre></div><h3 id="_3-生成-hubert-与-f0" tabindex="-1"><a class="header-anchor" href="#_3-生成-hubert-与-f0" aria-hidden="true">#</a> 3. 生成 hubert 与 f0</h3><div class="language-text ext-text"><pre class="language-text"><code>python preprocess_hubert_f0.py --f0_predictor dio
</code></pre></div><p>f0_predictor 拥有以下选择</p><div class="language-text ext-text"><pre class="language-text"><code>crepe
dio
pm
harvest
rmvpe
fcpe
</code></pre></div><p>如果训练集过于嘈杂，请使用 crepe 处理 f0</p><p>如果省略 f0_predictor 参数，默认值为 rmvpe</p><p>尚若需要浅扩散功能（可选），需要增加--use_diff 参数，比如</p><div class="language-text ext-text"><pre class="language-text"><code>python preprocess_hubert_f0.py --f0_predictor dio --use_diff
</code></pre></div><p><strong>加速预处理</strong> 如若您的数据集比较大，可以尝试添加<code>--num_processes</code>参数：</p><div class="language-text ext-text"><pre class="language-text"><code>python preprocess_hubert_f0.py --f0_predictor dio --use_diff --num_processes 8
</code></pre></div><p>所有的Workers会被自动分配到多个线程上</p><p>执行完以上步骤后 dataset 目录便是预处理完成的数据，可以删除 dataset_raw 文件夹了</p><h2 id="🏋️‍-训练" tabindex="-1"><a class="header-anchor" href="#🏋️‍-训练" aria-hidden="true">#</a> 🏋️‍ 训练</h2><h3 id="主模型训练" tabindex="-1"><a class="header-anchor" href="#主模型训练" aria-hidden="true">#</a> 主模型训练</h3><div class="language-text ext-text"><pre class="language-text"><code>python train.py -c configs/config.json -m 44k
</code></pre></div><h3 id="扩散模型-可选" tabindex="-1"><a class="header-anchor" href="#扩散模型-可选" aria-hidden="true">#</a> 扩散模型（可选）</h3><p>尚若需要浅扩散功能，需要训练扩散模型，扩散模型训练方法为：</p><div class="language-text ext-text"><pre class="language-text"><code>python train_diff.py -c configs/diffusion.yaml
</code></pre></div><p>模型训练结束后，模型文件保存在<code>logs/44k</code>目录下，扩散模型在<code>logs/44k/diffusion</code>下</p><p>如果前面的效果已经满意，或者没看明白下面在讲啥，那后面的内容都可以忽略，不影响模型使用(这些可选项影响比较小，可能在某些特定数据上有点效果，但大部分情况似乎都感知不太明显)</p><h2 id="模型效果增强" tabindex="-1"><a class="header-anchor" href="#模型效果增强" aria-hidden="true">#</a> 模型效果增强</h2><h3 id="自动-f0-预测" tabindex="-1"><a class="header-anchor" href="#自动-f0-预测" aria-hidden="true">#</a> 自动 f0 预测</h3><p>4.0 模型训练过程会训练一个 f0 预测器，对于语音转换可以开启自动音高预测，如果效果不好也可以使用手动的，但转换歌声时请不要启用此功能！！！会严重跑调！！</p><ul><li>在 inference_main 中设置 auto_predict_f0 为 true 即可</li></ul><h3 id="聚类音色泄漏控制" tabindex="-1"><a class="header-anchor" href="#聚类音色泄漏控制" aria-hidden="true">#</a> 聚类音色泄漏控制</h3><p>介绍：聚类方案可以减小音色泄漏，使得模型训练出来更像目标的音色（但其实不是特别明显），但是单纯的聚类方案会降低模型的咬字（会口齿不清）（这个很明显），本模型采用了融合的方式，可以线性控制聚类方案与非聚类方案的占比，也就是可以手动在&quot;像目标音色&quot; 和 &quot;咬字清晰&quot; 之间调整比例，找到合适的折中点</p><p>使用聚类前面的已有步骤不用进行任何的变动，只需要额外训练一个聚类模型，虽然效果比较有限，但训练成本也比较低</p><ul><li>训练过程： <ul><li>使用 cpu 性能较好的机器训练，据我的经验在腾讯云 6 核 cpu 训练每个 speaker 需要约 4 分钟即可完成训练</li><li>执行<code>python cluster/train_cluster.py</code>，模型的输出会在<code>logs/44k/kmeans_10000.pt</code></li><li>聚类模型目前可以使用 gpu 进行训练，执行<code>python cluster/train_cluster.py --gpu</code></li></ul></li></ul><div class="language-bash ext-sh"><pre class="language-bash"><code><span class="token comment"># CPU</span>
python cluster/train_cluster.py
<span class="token comment"># GPU</span>
python cluster/train_cluster.py --gpu
</code></pre></div><ul><li>推理过程： <ul><li><code>inference_main.py</code>中指定<code>cluster_model_path</code></li><li><code>inference_main.py</code>中指定<code>cluster_infer_ratio</code>，<code>0</code>为完全不使用聚类，<code>1</code>为只使用聚类，通常设置<code>0.5</code>即可</li></ul></li></ul><h3 id="特征检索" tabindex="-1"><a class="header-anchor" href="#特征检索" aria-hidden="true">#</a> 特征检索</h3><p>介绍：跟聚类方案一样可以减小音色泄漏，咬字比聚类稍好，但会降低推理速度，采用了融合的方式，可以线性控制特征检索与非特征检索的占比，</p><ul><li>训练过程： 首先需要在生成 hubert 与 f0 后执行：</li></ul><div class="language-bash ext-sh"><pre class="language-bash"><code>python train_index.py -c configs/config.json
</code></pre></div><p>模型的输出会在<code>logs/44k/feature_and_index.pkl</code></p><ul><li>推理过程： <ul><li>需要首先制定<code>--feature_retrieval</code>，此时聚类方案会自动切换到特征检索方案</li><li><code>inference_main.py</code>中指定<code>cluster_model_path</code> 为模型输出文件</li><li><code>inference_main.py</code>中指定<code>cluster_infer_ratio</code>，<code>0</code>为完全不使用特征检索，<code>1</code>为只使用特征检索，通常设置<code>0.5</code>即可</li></ul></li></ul><h2 id="🤖-推理" tabindex="-1"><a class="header-anchor" href="#🤖-推理" aria-hidden="true">#</a> 🤖 推理</h2><p>使用 <a href="https://github.com/svc-develop-team/so-vits-svc/blob/4.1-Stable/inference_main.py" target="_blank" rel="noopener noreferrer">inference_main.py</a></p><div class="language-text ext-text"><pre class="language-text"><code># 例
python inference_main.py -m &quot;logs/44k/G_30400.pth&quot; -c &quot;configs/config.json&quot; -n &quot;君の知らない物語-src.wav&quot; -t 0 -s &quot;nen&quot;
</code></pre></div><p>必填项部分：</p><ul><li><code>-m</code> | <code>--model_path</code>：模型路径</li><li><code>-c</code> | <code>--config_path</code>：配置文件路径</li><li><code>-n</code> | <code>--clean_names</code>：wav 文件名列表，放在 raw 文件夹下</li><li><code>-t</code> | <code>--trans</code>：音高调整，支持正负（半音）</li><li><code>-s</code> | <code>--spk_list</code>：合成目标说话人名称</li><li><code>-cl</code> | <code>--clip</code>：音频强制切片，默认 0 为自动切片，单位为秒/s</li></ul><p>可选项部分：部分具体见下一节</p><ul><li><code>-lg</code> | <code>--linear_gradient</code>：两段音频切片的交叉淡入长度，如果强制切片后出现人声不连贯可调整该数值，如果连贯建议采用默认值 0，单位为秒</li><li><code>-f0p</code> | <code>--f0_predictor</code>：选择 F0 预测器，可选择 crepe,pm,dio,harvest,rmvpe,fcpe, 默认为 pm（注意：crepe 为原 F0 使用均值滤波器）</li><li><code>-a</code> | <code>--auto_predict_f0</code>：语音转换自动预测音高，转换歌声时不要打开这个会严重跑调</li><li><code>-cm</code> | <code>--cluster_model_path</code>：聚类模型或特征检索索引路径，留空则自动设为各方案模型的默认路径，如果没有训练聚类或特征检索则随便填</li><li><code>-cr</code> | <code>--cluster_infer_ratio</code>：聚类方案或特征检索占比，范围 0-1，若没有训练聚类模型或特征检索则默认 0 即可</li><li><code>-eh</code> | <code>--enhance</code>：是否使用 NSF_HIFIGAN 增强器，该选项对部分训练集少的模型有一定的音质增强效果，但是对训练好的模型有反面效果，默认关闭</li><li><code>-shd</code> | <code>--shallow_diffusion</code>：是否使用浅层扩散，使用后可解决一部分电音问题，默认关闭，该选项打开时，NSF_HIFIGAN 增强器将会被禁止</li><li><code>-usm</code> | <code>--use_spk_mix</code>：是否使用角色融合/动态声线融合</li><li><code>-lea</code> | <code>--loudness_envelope_adjustment</code>：输入源响度包络替换输出响度包络融合比例，越靠近 1 越使用输出响度包络</li><li><code>-fr</code> | <code>--feature_retrieval</code>：是否使用特征检索，如果使用聚类模型将被禁用，且 cm 与 cr 参数将会变成特征检索的索引路径与混合比例</li></ul><p>浅扩散设置：</p><ul><li><code>-dm</code> | <code>--diffusion_model_path</code>：扩散模型路径</li><li><code>-dc</code> | <code>--diffusion_config_path</code>：扩散模型配置文件路径</li><li><code>-ks</code> | <code>--k_step</code>：扩散步数，越大越接近扩散模型的结果，默认 100</li><li><code>-od</code> | <code>--only_diffusion</code>：纯扩散模式，该模式不会加载 sovits 模型，以扩散模型推理</li><li><code>-se</code> | <code>--second_encoding</code>：二次编码，浅扩散前会对原始音频进行二次编码，玄学选项，有时候效果好，有时候效果差</li></ul><h3 id="注意-1" tabindex="-1"><a class="header-anchor" href="#注意-1" aria-hidden="true">#</a> 注意！</h3><p>如果使用<code>whisper-ppg</code> 声音编码器进行推理，需要将<code>--clip</code>设置为 25，<code>-lg</code>设置为 1。否则将无法正常推理。</p><h3 id="使用webui推理" tabindex="-1"><a class="header-anchor" href="#使用webui推理" aria-hidden="true">#</a> 使用webUI推理</h3><div class="language-bash ext-sh"><pre class="language-bash"><code>python webUI.py
</code></pre></div><p>浏览器打开推理的webui界面，上传模型和config文件(必须项)，扩散模型和配置可选上传，然后加载模型。使用文本转语音功能最好开启f0预测。</p><h2 id="人声伴奏分离" tabindex="-1"><a class="header-anchor" href="#人声伴奏分离" aria-hidden="true">#</a> 人声伴奏分离</h2><p>使用工具**<a href="https://ultimatevocalremover.com/" target="_blank" rel="noopener noreferrer">ULTIMATE VOCAL REMOVER</a>**进行纯净的人声分离，可用于人声训练集，或者用于歌曲干声推理工程。</p><p>GUI地址：https://github.com/Anjok07/ultimatevocalremovergui。</p><p>提取人声推荐使用5_HP-Karaoke-UVR这个模型，分离得比较干净。</p><p>提取伴奏推荐使用7_HP2-UVR。</p><p><img src="/img/in-post/2023-11-30/1.jpg" alt="1"></p><ul><li><p>选择输入文件和输出文件夹</p></li><li><p>勾选CPU Conversion，Instrumental是提取伴奏，Vocals是提取人声。</p></li><li><p>选择模型。</p><blockquote><p>如果没有模型，点击扳手图标，Download Center下载对应模型。</p><p>下载不了的话，可以点Manual Download，从浏览器下载，放到models文件夹里即可。</p></blockquote></li><li><p>点Start Processing开始分离。根据你GPU的性能来决定进程速度。</p></li></ul><h2 id="继续训练" tabindex="-1"><a class="header-anchor" href="#继续训练" aria-hidden="true">#</a> 继续训练</h2><p>如果觉得训练的步数过少，可以选择继续训练，把训练文件： <code>G_20000.pth</code> <code>D_20000.pth</code>放在<code>logs/44k</code>目录下，然后执行训练代码就可以继续训练。</p><p><strong>如果想要训练其他的模型，首先删除之前的项目文件夹，重新克隆git仓库项目，由于环境已经配置好，不需要再重新配置环境，进入conda的python环境，导入训练集按照上方配置项目重新进行训练即可。</strong></p></div><!--[--><!--]--></div><footer class="page-meta"><div class="meta-item edit-link"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg><a class="external-link meta-item-label" href="https://github.com/Mrzhuo2022/Mrzhuo2022.github.io/edit/main/blog/posts/2023-11-30-so-vits-svc-doc.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page on GitHub"><!--[--><!--]--><!----><span>Edit this page on GitHub</span><!--[--><!--]--></a></div><div class="meta-item last-updated"><span class="meta-item-label">Last Updated: </span><!----></div><!----></footer><!----><!--[--><div class="pager"><a href="/post/2023/10/14/Linux-and-wsl/" class="prev">Previous <br><span>Linux基础和wsl安装</span></a><a href="/post/2025/07/18/new-VPS-guide/" class="next">Next <br><span>拿到新的小鸡（VPS）后，你应该先做什么？</span></a></div><!--]--><!----></main><ul class="catalog" style="top:0px;"><li class="level-2 toc-link-模型简介">模型简介</li><li class="level-2 toc-link-使用规约">使用规约</li><li class="level-2 toc-link-安装wsl2和conda">安装wsl2和conda</li><li class="level-3 toc-link-为什么用linux系统来训练ai模型">为什么用Linux系统来训练AI模型</li><li class="level-3 toc-link-为什么使用conda">为什么使用conda</li><li class="level-2 toc-link-环境依赖准备">环境依赖准备</li><li class="level-3 toc-link-python">Python</li><li class="level-3 toc-link-cuda">CUDA</li><li class="level-3 toc-link-pytorch">Pytorch</li><li class="level-3 toc-link-测试环境">测试环境</li><li class="level-2 toc-link-配置项目">配置项目</li><li class="level-2 toc-link-预下载模型文件">预下载模型文件</li><li class="level-4 toc-link-必须项">必须项</li><li class="level-5 toc-link-_1-若使用-contentvec-作为声音编码器-推荐">1. 若使用 contentvec 作为声音编码器（推荐）</li><li class="level-5 toc-link-_2-若使用-hubertsoft-作为声音编码器">2. 若使用 hubertsoft 作为声音编码器</li><li class="level-5 toc-link-_3-若使用-whisper-ppg-作为声音编码器">3. 若使用 Whisper-ppg 作为声音编码器</li><li class="level-5 toc-link-_4-若使用-cnhubertlarge-作为声音编码器">4. 若使用 cnhubertlarge 作为声音编码器</li><li class="level-5 toc-link-_5-若使用-dphubert-作为声音编码器">5. 若使用 dphubert 作为声音编码器</li><li class="level-5 toc-link-_6-若使用-onnxhubert-contentvec-作为声音编码器">6. 若使用 OnnxHubert/ContentVec 作为声音编码器</li><li class="level-4 toc-link-编码器列表">编码器列表</li><li class="level-4 toc-link-可选项-强烈建议使用">可选项(强烈建议使用)</li><li class="level-2 toc-link-数据集准备">数据集准备</li><li class="level-2 toc-link-🛠️-数据预处理">🛠️ 数据预处理</li><li class="level-3 toc-link-_0-音频切片">0. 音频切片</li><li class="level-3 toc-link-_1-重采样至-44100hz-单声道">1. 重采样至 44100Hz 单声道</li><li class="level-4 toc-link-注意">注意</li><li class="level-3 toc-link-_2-自动划分训练集、验证集-以及自动生成配置文件">2. 自动划分训练集、验证集，以及自动生成配置文件</li><li class="level-4 toc-link-此时可以在生成的-config-json-与-diffusion-yaml-修改部分参数">此时可以在生成的 config.json 与 diffusion.yaml 修改部分参数</li><li class="level-5 toc-link-config-json">config.json</li><li class="level-5 toc-link-diffusion-yaml">diffusion.yaml</li><li class="level-5 toc-link-声码器列表">声码器列表</li><li class="level-3 toc-link-_3-生成-hubert-与-f0">3. 生成 hubert 与 f0</li><li class="level-2 toc-link-🏋️‍-训练">🏋️‍ 训练</li><li class="level-3 toc-link-主模型训练">主模型训练</li><li class="level-3 toc-link-扩散模型-可选">扩散模型（可选）</li><li class="level-2 toc-link-模型效果增强">模型效果增强</li><li class="level-3 toc-link-自动-f0-预测">自动 f0 预测</li><li class="level-3 toc-link-聚类音色泄漏控制">聚类音色泄漏控制</li><li class="level-3 toc-link-特征检索">特征检索</li><li class="level-2 toc-link-🤖-推理">🤖 推理</li><li class="level-3 toc-link-注意-1">注意！</li><li class="level-3 toc-link-使用webui推理">使用webUI推理</li><li class="level-2 toc-link-人声伴奏分离">人声伴奏分离</li><li class="level-2 toc-link-继续训练">继续训练</li></ul></div><!--]--></div><div class="search-page" role="search"><span class="search-close"><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512" width="28" height="28" fill="currentColor"><path d="M224 416c-8.188 0-16.38-3.125-22.62-9.375l-192-192c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L224 338.8l169.4-169.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-192 192C240.4 412.9 232.2 416 224 416z"></path></svg></span><div class="gungnir-search-box"><input placeholder="$ grep ..." autocomplete="off" spellcheck="false" value><!----></div></div><div class="menu-btn-container"><div class="menu-btn-wrapper"><div class="menu-btn"><div style="" class="menu-btn-icon"><span></span><span></span><span></span></div><div style="display:none;" class="menu-text">0</div><svg class="menu-progress"><circle class="menu-border" cx="50%" cy="50%" r="48%" style="stroke-dasharray:0% 314.15926%;"></circle></svg></div><div class="menu-btn-child-wrapper"><div title="toggle color mode" class="menu-btn-child"><svg class="ov-icon" style="font-size:1.2em;display:none;" aria-hidden="true" width="19.2" height="19.2" viewbox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M256 160c-52.9 0-96 43.1-96 96s43.1 96 96 96 96-43.1 96-96-43.1-96-96-96zm246.4 80.5l-94.7-47.3 33.5-100.4c4.5-13.6-8.4-26.5-21.9-21.9l-100.4 33.5-47.4-94.8c-6.4-12.8-24.6-12.8-31 0l-47.3 94.7L92.7 70.8c-13.6-4.5-26.5 8.4-21.9 21.9l33.5 100.4-94.7 47.4c-12.8 6.4-12.8 24.6 0 31l94.7 47.3-33.5 100.5c-4.5 13.6 8.4 26.5 21.9 21.9l100.4-33.5 47.3 94.7c6.4 12.8 24.6 12.8 31 0l47.3-94.7 100.4 33.5c13.6 4.5 26.5-8.4 21.9-21.9l-33.5-100.4 94.7-47.3c13-6.5 13-24.7.2-31.1zm-155.9 106c-49.9 49.9-131.1 49.9-181 0-49.9-49.9-49.9-131.1 0-181 49.9-49.9 131.1-49.9 181 0 49.9 49.9 49.9 131.1 0 181z"/></svg><svg class="ov-icon" style="font-size:1.2em;display:none;" aria-hidden="true" width="19.2" height="19.2" viewbox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M283.211 512c78.962 0 151.079-35.925 198.857-94.792 7.068-8.708-.639-21.43-11.562-19.35-124.203 23.654-238.262-71.576-238.262-196.954 0-72.222 38.662-138.635 101.498-174.394 9.686-5.512 7.25-20.197-3.756-22.23A258.156 258.156 0 00283.211 0c-141.309 0-256 114.511-256 256 0 141.309 114.511 256 256 256z"/></svg><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M224 96l16-32 32-16-32-16-16-32-16 32-32 16 32 16 16 32zM80 160l26.66-53.33L160 80l-53.34-26.67L80 0 53.34 53.33 0 80l53.34 26.67L80 160zm352 128l-26.66 53.33L352 368l53.34 26.67L432 448l26.66-53.33L512 368l-53.34-26.67L432 288zm70.62-193.77L417.77 9.38C411.53 3.12 403.34 0 395.15 0c-8.19 0-16.38 3.12-22.63 9.38L9.38 372.52c-12.5 12.5-12.5 32.76 0 45.25l84.85 84.85c6.25 6.25 14.44 9.37 22.62 9.37 8.19 0 16.38-3.12 22.63-9.37l363.14-363.15c12.5-12.48 12.5-32.75 0-45.24zM359.45 203.46l-50.91-50.91 86.6-86.6 50.91 50.91-86.6 86.6z"/></svg></div><div class="menu-btn-child"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="-75.52 -43.52 599.04 599.04" fill="currentColor"><path d="M207.029 381.476L12.686 187.132c-9.373-9.373-9.373-24.569 0-33.941l22.667-22.667c9.357-9.357 24.522-9.375 33.901-.04L224 284.505l154.745-154.021c9.379-9.335 24.544-9.317 33.901.04l22.667 22.667c9.373 9.373 9.373 24.569 0 33.941L240.971 381.476c-9.373 9.372-24.569 9.372-33.942 0z"/></svg></div><div class="menu-btn-child"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="-75.52 -43.52 599.04 599.04" fill="currentColor"><path d="M240.971 130.524l194.343 194.343c9.373 9.373 9.373 24.569 0 33.941l-22.667 22.667c-9.357 9.357-24.522 9.375-33.901.04L224 227.495 69.255 381.516c-9.379 9.335-24.544 9.317-33.901-.04l-22.667-22.667c-9.373-9.373-9.373-24.569 0-33.941L207.03 130.525c9.372-9.373 24.568-9.373 33.941-.001z"/></svg></div><div class="menu-btn-child menu-toc-btn"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M48 48a48 48 0 1048 48 48 48 0 00-48-48zm0 160a48 48 0 1048 48 48 48 0 00-48-48zm0 160a48 48 0 1048 48 48 48 0 00-48-48zm448 16H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16v-32a16 16 0 00-16-16zm0-320H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16V80a16 16 0 00-16-16zm0 160H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16v-32a16 16 0 00-16-16z"/></svg></div><div class="toggle-sidebar-button menu-btn-child menu-btn-sidebar" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="-1.6 -1.6 19.2 19.2" fill="currentColor"><path d="M14 2a1 1 0 011 1v10a1 1 0 01-1 1H2a1 1 0 01-1-1V3a1 1 0 011-1h12zM2 1a2 2 0 00-2 2v10a2 2 0 002 2h12a2 2 0 002-2V3a2 2 0 00-2-2H2z"/><path d="M3 4a1 1 0 011-1h2a1 1 0 011 1v8a1 1 0 01-1 1H4a1 1 0 01-1-1V4z"/></svg></div></div></div></div><footer class="footer"><span>
      &copy;2023 <a href="#" target="_blank">Evarle</a>
      <br>
      Powered by <a href="https://v2.vuepress.vuejs.org" target="_blank">VuePress</a> &
      <a href="https://github.com/Renovamen/vuepress-theme-gungnir" target="_blank">Gungnir</a>
    </span></footer></div><!--]--></div>
    <script type="module" src="/assets/app.1ce497e0.js" defer></script>
  </body>
</html>
